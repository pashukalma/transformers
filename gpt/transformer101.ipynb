{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w91wFHeREgf"
   },
   "source": [
    "**Processing Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fat5CpKvX8D3"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzuUpkFtZ2Do",
    "outputId": "35b67f1e-c2e4-4290-84b8-0cd598996eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guttenberg2.txt  guttenberg-ch1.txt  guttenberg-fr-ch1.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMYmvfItZ-_M",
    "outputId": "5c662c34-905e-42cc-bbd6-e27e08b31e49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Chapter 1\\n\\n\\nHappy families are', 1966150)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('guttenberg2.txt', 'r', encoding='utf-8') as filedesc:\n",
    "  text = filedesc.read()\n",
    "text[:30], len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezuYuumIZ_K9",
    "outputId": "3dae2989-208c-434d-90a5-b5c28b757689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chapter 1   happy families are', 2095240)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_text = text.lower().replace(\"\\n\", \" \")\n",
    "for chr in \",.:;?!$()/_&%*@'`\":\n",
    "  c_text = c_text.replace(f'{chr}', f' {chr} ')\n",
    "c_text[:30], len(c_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruTXFMqsZ_Hh",
    "outputId": "7e03c725-3418-418f-9631-1f78a1c4e188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Chapter', ' ', '1', '\\n', '', '\\n', '', '\\n', 'Happy', ' '], 876633)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "with open('guttenberg2.txt', 'r', encoding='utf-8') as filedesc:\n",
    "  text = filedesc.read()\n",
    "text_processing = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "text_processing[:10], len(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TkgRDIZa-TT",
    "outputId": "2bac9a2b-215c-492f-993b-227eaba9f7be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter', '1', 'Happy', 'families', 'are', 'all', 'alike']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processing = [i.strip() for i in text_processing if i.strip()]\n",
    "text_processing[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZnCWoKMb8nr",
    "outputId": "b5d544bd-5338-493e-e4b6-408f8188aea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14167,)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = sorted(set(text_processing))\n",
    "vocabulary_size = len(allwords)\n",
    "vocabulary = {word_: integer_ for integer_, word_ in enumerate(allwords)}\n",
    "vocabulary_size,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VhfajVAb8xC",
    "outputId": "0a53e9f8-7b47-47cf-833e-1c0e7dae7cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0) ('\"', 1) (\"'\", 2) ('(', 3) (')', 4) (',', 5) ('--', 6) ('-`', 7) ('.', 8) ('015', 9) ('038', 10) ('1', 11) ('10', 12) ('11', 13) ('12', 14) ('13', 15) ('14', 16) ('15', 17) ('16', 18) ('17', 19) ('17th', 20) ('18', 21) ('1863', 22) ('1864', 23) ('19', 24) ('2', 25) ('20', 26) ('21', 27) ('22', 28) ('23', 29) ('24', 30) ('25', 31) ('26', 32) ('27', 33) ('28', 34) ('29', 35) ('2nd', 36) ('3', 37) ('30', 38) ('30th', 39) ('31', 40) ('32', 41) ('33', 42) ('34', 43) ('35', 44) ('36', 45) ('4', 46) ('5', 47) ('6', 48) ('7', 49) ('8', 50) "
     ]
    }
   ],
   "source": [
    "for key, item in enumerate(vocabulary.items()):\n",
    "  print(item, end =' ')\n",
    "  if key >=50: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxw8LblsRNLe"
   },
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYvxtbIpb9et",
    "outputId": "5f1f6013-290b-47f5-85a0-7d431d0b69eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alphonse', 100), ('Already', 101), ('Although', 102), ('Altogether', 103)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords = sorted(list(set(text_processing)))\n",
    "allwords.extend([\"<|END|>\", '<|unk|>'])\n",
    "vocabulary = {word_: integer_ for integer_, word_ in enumerate(allwords)}\n",
    "list(vocabulary.items())[100:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6O-WBWOb81e"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.str_to_int = vocabulary\n",
    "    self.int_to_str = {int_: str_ for str_, int_ in vocabulary.items()}\n",
    "  def encode(self, text):\n",
    "    text_pre = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    text_pre = [item.strip() for item in text_pre if item.strip() ]\n",
    "    text_pre = [item if item in self.str_to_int else '<|unk|>' for item in text_pre]\n",
    "    ids = [self.str_to_int[str_] for str_ in text_pre]\n",
    "    return ids\n",
    "  def decode(self, ids):\n",
    "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
    "    text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWHUpEMib9Gc",
    "outputId": "2e3c08e5-fea4-4cb9-82ae-05fa572b954a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter', ' ', '1', ' ', 'Happy', ' ', 'families', ' ', 'are', ' ']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocabulary)\n",
    "ids = tokenizer.encode(text)\n",
    "tokenizer.decode(ids)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLAZD_cWb9ar"
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqBASXAEk3q3"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncStN-cWb9iw",
    "outputId": "acca1787-47ad-40e0-8940-085503180eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " [198, 464, 4935, 20336, 46566, 220, 50256, 764],\n",
       " '\\nThe Project Gutenberg eBook <|endoftext|> .')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_g = ('''\n",
    "The Project Gutenberg eBook <|endoftext|> of Anna Karenina, by Leo Tolstoy\n",
    "\n",
    "This eBook is for the use of anyone anywhere in the United States and\n",
    "most other parts of the world at no cost and with almost no restrictions\n",
    "whatsoever. You may copy it, give it away or re-use it under the terms\n",
    "of the Project Gutenberg License included with this eBook or online at\n",
    "www.gutenberg.org. If you are not located in the United States, you\n",
    "will have to check the laws of the country where you are located before\n",
    "using this eBook.''')\n",
    "ints = tokenizer.encode(text_g, allowed_special={\"<|endoftext|>\"})\n",
    "strings = tokenizer.decode(ints)\n",
    "len(ints), ints, strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46zFcxVkb9pB"
   },
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(text_g, allowed_special={\"<|endoftext|>\"})\n",
    "encoded_sample = encoded_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rgxvJAXvJNV"
   },
   "source": [
    "**Sample batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAfuoxOmZ_OC",
    "outputId": "6d8b4e27-0107-4a77-b538-bf39114a41fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8203, 4548, 9196, 12762], [4548, 9196, 12762])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = encoded_sample[: context_size]\n",
    "y = encoded_sample[1: context_size]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esB9zt4Gijtm",
    "outputId": "9970fa99-89c0-46d4-908e-eeb8919b4032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8203]  ---  4548\n",
      "[8203, 4548]  ---  9196\n",
      "[8203, 4548, 9196]  ---  12762\n",
      "[8203, 4548, 9196, 12762]  ---  12607\n",
      "['little']  ---  ['decanters']\n",
      "['little', ' ', 'decanters']  ---  ['on']\n",
      "['little', ' ', 'decanters', ' ', 'on']  ---  ['the']\n",
      "['little', ' ', 'decanters', ' ', 'on', ' ', 'the']  ---  ['table']\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size +1):\n",
    "  print(encoded_sample[:i], ' --- ', encoded_sample[i])\n",
    "\n",
    "for i in range(1, context_size +1):\n",
    "  print(tokenizer.decode(encoded_sample[:i]), ' --- ',\n",
    "          tokenizer.decode([encoded_sample[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYB_PuzPQwig"
   },
   "source": [
    "**Dataset and DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSmkTsEUrmR8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ghm8UlooijxB"
   },
   "outputs": [],
   "source": [
    "class Dataset_v1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "\n",
    "    for i in range(0, len(token_ids) -max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i+1:i + max_length +1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dWdoFwUij33"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset_v1(text, tokenizer, max_length=256, stride=128)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9I0qCjKtNbT"
   },
   "outputs": [],
   "source": [
    "def DataLoader_v1(txt, batch_size = 4,\n",
    "                  max_length = 256, stride = 128,\n",
    "                  shuffle = True, drop_last = True, num_workers = 0):\n",
    "  tokenizer = tiktoken.get_encoding('gpt2')\n",
    "  dataset = Dataset_v1(text, tokenizer, max_length, stride)\n",
    "  dataloader = DataLoader(dataset,\n",
    "    batch_size = batch_size, shuffle = True, drop_last = True, num_workers = 0)\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uti2OgJTij-K",
    "outputId": "96147b9e-f863-419e-a0ad-f1ab8c7a4eb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1046,    72, 13557,  ...,    13,   198,   198],\n",
       "         [21000, 18657,  2007,  ...,    13,   198,   198],\n",
       "         [12659,  1290,    12,  ...,   262,  5156,   338],\n",
       "         [  284,   307,   826,  ...,  5779,    11,  3763]]),\n",
       " tensor([[   72, 13557,  1318,  ...,   198,   198,     1],\n",
       "         [18657,  2007,   750,  ...,   198,   198,     1],\n",
       "         [ 1290,    12,  8272,  ...,  5156,   338,   736],\n",
       "         [  307,   826,    11,  ...,    11,  3763,     0]])]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' recap each step from the preprocessing '''\n",
    "with open('guttenberg2.txt', 'r', encoding='utf-8') as filedesc:\n",
    "  new_text = filedesc.read()\n",
    "\n",
    "dataloader = DataLoader_v1(new_text, batch_size=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrwYsoYmxd9a",
    "outputId": "a7ac48e1-3aff-4b17-f7bd-02b19effbbb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6830,   373, 11069,  ...,   507,    11,   996],\n",
       "         [  262,   649,  4039,  ...,  5012,   272,  9128],\n",
       "         [  503,   257,  8011,  ...,  3612,    11,   290],\n",
       "         [  619,    82,   198,  ...,  2993,   523, 42075]]),\n",
       " tensor([[  373, 11069,   284,  ...,    11,   996,  3360],\n",
       "         [  649,  4039,  1244,  ...,   272,  9128,  4597],\n",
       "         [  257,  8011,    11,  ...,    11,   290,   339],\n",
       "         [   82,   198,   392,  ...,   523, 42075,   284]]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_, target_ = next(data_iter)\n",
    "input_, target_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIyBubl5xUVo"
   },
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9s9Lc7bKL2p"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Convert text into numerical vectors, embeddings, transform discrete data,\n",
    "text -images = more into vector spaces, process text into tokesn with tokenizer\n",
    "class, enhance the model's understanding\n",
    "BPE tokenizer used in GPT-3 efficiently handles this by breaking down words\n",
    "into individual characters\n",
    "Use sliding window on tokenized data to generate input-target pairs\n",
    "Embedding layer in Pytorch is lookup operation given the token\n",
    "While token embeddings provide consistent vector representations to capture\n",
    "meaning and semantics we need the positioning (positional embedding) and add it\n",
    "tothe token embedding which is optimized during training\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wB3XtTYjxFJA",
    "outputId": "20c4f80e-46e0-4997-8811-c8d7750e201f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.7416, -0.3635,  0.9803,  ...,  0.5644, -0.8004,  1.5482],\n",
       "        [-1.1733, -0.3457, -0.6575,  ...,  0.9468,  0.8291,  0.6873],\n",
       "        [ 0.5252,  0.4847, -0.2217,  ..., -2.1106,  0.2912, -0.4572],\n",
       "        ...,\n",
       "        [ 0.4858, -0.7001,  0.4734,  ..., -0.8532, -2.0940,  0.6317],\n",
       "        [ 1.4763, -0.7158,  3.0351,  ..., -0.9276,  1.3366,  1.1988],\n",
       "        [-0.3399,  0.6682, -1.5056,  ..., -0.9517,  0.3251,  0.4580]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "output_dim = 256\n",
    "embedding_ = torch.nn.Embedding(vocabulary_size, output_dim)\n",
    "embedding_.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIYRbhQ-xFNA"
   },
   "outputs": [],
   "source": [
    "embedding_(torch.tensor([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACdRDRUpxFQk",
    "outputId": "8e537bb8-994f-4828-bd28-14efc40466fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(14167, 256), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_, input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zd6f7h4G3j3i"
   },
   "outputs": [],
   "source": [
    "with open('guttenberg2.txt', 'r', encoding='utf-8') as filedesc:\n",
    "  new_text = filedesc.read()\n",
    "\n",
    "dataloader = DataLoader_v1(\n",
    "    new_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "batch_1 = next(data_iter)\n",
    "batch_2 =  next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7N23WwUjxGQ3",
    "outputId": "adc31c1e-34e9-40ff-9a79-cb97830e6000"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2342,   319,   644,   198],\n",
       "         [ 1169, 38265,   293,    11],\n",
       "         [   11,   290,  6150,   284],\n",
       "         [  351, 34730,   290, 47886],\n",
       "         [  284,   607,    13,  1375],\n",
       "         [  345,   644,   345,  3551],\n",
       "         [  198,    67,   570,  1431],\n",
       "         [  477,   286,   606,   547]]),\n",
       " tensor([[  319,   644,   198,   258],\n",
       "         [38265,   293,    11,  3538],\n",
       "         [  290,  6150,   284, 15849],\n",
       "         [34730,   290, 47886,  2951],\n",
       "         [  607,    13,  1375,  2497],\n",
       "         [  644,   345,  3551,   546],\n",
       "         [   67,   570,  1431,   355],\n",
       "         [  286,   606,   547,  1035]]))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_, target_ = batch_1\n",
    "input_, target_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dSDfIkBxGNt",
    "outputId": "232d127f-9c56-4b74-86c3-b8b530427f08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 256)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(132)\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7V9W8E8zJOmC",
    "outputId": "a2918da9-e65c-4633-fab9-967114a87b78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding_layer = embedding_layer(input_)\n",
    "token_embedding_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5K8zOwX7xGUV",
    "outputId": "30a0325c-b69c-4280-f85a-1f8a3383ffc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max_length = 4\n",
    "positional_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "positional_embedding_layer = embedding_layer(torch.arange(context_length))\n",
    "positional_embedding_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZzBF2fPxGXv",
    "outputId": "7936d8dc-20ab-472a-82e3-75bbb94e53cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding = token_embedding_layer + positional_embedding_layer\n",
    "input_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7XSJJsFQpd5"
   },
   "source": [
    "**Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTTIMqbaEQXd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basic self-attention framework in neural networks, causal attention module that\n",
    "allows models to generate one token at a time, and masking randomly selected\n",
    "attention weights with dropout to reduce overfitting\n",
    "Stacking multiple causal attention modules into multi-head attention\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mj_USCzfEQau",
    "outputId": "f6c035a5-a658-4ef3-ad6d-6d4a4ab520b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 3]), torch.Size([6, 3]))"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' simplified version '''\n",
    "input = input_a.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOhdfEdIEQlq"
   },
   "outputs": [],
   "source": [
    "query = input[1]\n",
    "attention_score = torch.empty(input.shape[0])\n",
    "for i, x_i in enumerate(input):\n",
    "  attention_score[i] = torch.dot(x_i, query)\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-_iAG9rEQpc"
   },
   "outputs": [],
   "source": [
    "''' attention scores as a dot product of inputs '''\n",
    "dotProduct =  torch.dot(input[0], query)\n",
    "dotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xstuo5viaBhB"
   },
   "outputs": [],
   "source": [
    "''' attention weights the normalized version of attention scores '''\n",
    "attention_weights_tmp = attention_score/attention_score.sum()\n",
    "attention_weights_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmhozltOaBks"
   },
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "attention_weights_naive = softmax_naive(attention_score)\n",
    "attention_weights_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME3UD8j4aBnn"
   },
   "outputs": [],
   "source": [
    "''' context vectors as weighted sum over inputs '''\n",
    "query = input[1]\n",
    "attention_weights = torch.softmax(attention_score, dim=0)\n",
    "context_vector = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(input):\n",
    "  context_vector += attention_score[i] *x_i\n",
    "context_vector, attention_weights, attention_weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfKSv5AiexZ2"
   },
   "source": [
    "Generalize these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFxsxZ4YxGas"
   },
   "outputs": [],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i,x_i in enumerate(input):\n",
    "  for j,x_j in enumerate(input):\n",
    "    attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSBBWwdpikBA"
   },
   "outputs": [],
   "source": [
    "attn_scores_2 = input @input.T\n",
    "attn_scores_2 == attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djWrW3PaikEd"
   },
   "outputs": [],
   "source": [
    "attention_weights = torch.softmax(attn_scores, dim=-1)\n",
    "all_context_vectors = attention_weights @ input\n",
    "all_context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMueKrGNfq-c"
   },
   "outputs": [],
   "source": [
    "''' self-attention with trainable weights '''\n",
    "x_2 = input[1]\n",
    "dim_in_emb = input.shape[1]\n",
    "dim_out_emb = 2\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(\n",
    "    torch.rand(dim_in_emb, dim_out_emb, requires_grad=False))\n",
    "W_key = torch.nn.Parameter(\n",
    "    torch.rand(dim_in_emb, dim_out_emb, requires_grad=False))\n",
    "W_value = torch.nn.Parameter(\n",
    "    torch.rand(dim_in_emb, dim_out_emb, requires_grad=False))\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "keys_2 = x_2 @ W_key\n",
    "values_2 = x_2 @ W_value\n",
    "\n",
    "keys = input @W_key\n",
    "values = input @ W_value\n",
    "\n",
    "attention_score_2 = query_2 @ keys.T\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_score_2/d_k**0.5, dim=-1)\n",
    "\n",
    "contenxt_vectors_2 = attention_weights_2 @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKhsKIsafrBt"
   },
   "outputs": [],
   "source": [
    "query_2, keys_2, values_2, keys.shape, values.shape, \\\n",
    "attention_score_2, attention_weights_2, contenxt_vectors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_iVJnsmjSdl"
   },
   "outputs": [],
   "source": [
    "''' wrap all this functionality in the self-attention class '''\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "  def __init__(self, dim_in_emb, dim_out_emb):\n",
    "    super().__init__()\n",
    "    self.W_query = nn.Parameter(torch.rand(dim_in_emb, dim_out_emb))\n",
    "    self.W_key = nn.Parameter(torch.rand(dim_in_emb, dim_out_emb))\n",
    "    self.W_value = nn.Parameter(torch.rand(dim_in_emb, dim_out_emb))\n",
    "\n",
    "  def forward(self, x):\n",
    "    queries = x @ self.W_query\n",
    "    keys = x @ self.W_key\n",
    "    values = x @ self.W_value\n",
    "    attn_score = queries @ keys.T\n",
    "    attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "    contenxt_vectors = attn_weights @ values\n",
    "    return contenxt_vectors\n",
    "\n",
    "torch.manual_seed(132)\n",
    "self_attn = SelfAttention_v1(dim_in_emb, dim_out_emb)\n",
    "self_attn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idI6RqWPj-CX"
   },
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "  def __init__(self, dim_in_emb, dim_out_emb, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.W_query = nn.Linear(torch.rand(dim_in_emb, dim_out_emb, bias=qkv_bias))\n",
    "    self.W_key = nn.Linear(torch.rand(dim_in_emb, dim_out_emb, bias=qkv_bias))\n",
    "    self.W_value = nn.Linear(torch.rand(dim_in_emb, dim_out_emb, bias=qkv_bias))\n",
    "\n",
    "  def forward(self, x):\n",
    "    queries = x @ W_query\n",
    "    keys = x @ W_key\n",
    "    values = x @ W_value\n",
    "    attn_score = queries @ keys.T\n",
    "    attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "    contenxt_vectors = attn_weights @ values\n",
    "    return contenxt_vectors\n",
    "\n",
    "torch.manual_seed(132)\n",
    "self_attn = SelfAttention_v2(dim_in_emb, dim_out_emb)\n",
    "self_attn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH6EdjVuj-F6"
   },
   "outputs": [],
   "source": [
    "class CausalAttention_v1(nn.Module):\n",
    "  def __init__(self, dim_in_emb, dim_out_emb,\n",
    "               context_length, dropout, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.W_query = nn.Linear(dim_in_emb, dim_out_emb, bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(dim_in_emb, dim_out_emb, bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(dim_in_emb, dim_out_emb, bias=qkv_bias)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(\n",
    "            context_length, context_length), diagonal=1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, num_tokens, dim_in_emb = x.shape\n",
    "    queries = x @ W_query\n",
    "    keys = x @ W_key\n",
    "    values = x @ W_value\n",
    "\n",
    "    attn_score = queries @ keys.transpose(1,2)\n",
    "    attn_score.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "    attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "    contenxt_vectors = attn_weights @ values\n",
    "    return contenxt_vectors\n",
    "\n",
    "torch.manual_seed(132)\n",
    "batch = torch.stack((input, input), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "ca_attn = CausalAttention_v1(dim_in_emb, dim_out_emb, context_length, 0.0)\n",
    "context_vectors = ca_attn(batch)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCcOo4oFj-Ja"
   },
   "outputs": [],
   "source": [
    "''' Multi-head attention layer '''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, dim_in_emb, dim_out_emb, context_length,\n",
    "               dropout, num_heads, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList(\n",
    "        [CausalAttention_v1(\n",
    "          dim_in_emb, dim_out_emb, context_length, dropout, qkv_bias)\n",
    "        for _ in range(num_heads)]\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "torch.manual_seed(132)\n",
    "context_length = batch.shape[1]\n",
    "dim_in_emb, dim_out_emb = 3, 2\n",
    "mha_ = MultiHeadAttention(\n",
    "    dim_in_emb, dim_out_emb, context_length, 0.0, num_heads=2)\n",
    "context_vectors = mha_(batch)\n",
    "context_vectors.shape, #context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDBgeFeg098F"
   },
   "outputs": [],
   "source": [
    "''' Multi-head attention with weights split '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLC_gqqO0-CC"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention_opt(nn.Module):\n",
    "  def __init__(self, dim_in_emb, dim_out_emb, context_length, dropout, num_heads,\n",
    "               qkv_bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dim_out_emb = dim_out_emb\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = dim_out_emb // num_heads\n",
    "\n",
    "    self.W_query = nn.Linear(dim_in_emb, dim_out_emb, bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(dim_in_emb, dim_out_emb, bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(dim_in_emb, dim_out_emb, bias=qkv_bias)\n",
    "    self.out_proj = nn.Linear(dim_out_emb, dim_out_emb)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, num_tokens, dim_in_emb = x.shape\n",
    "\n",
    "    queries = self.W_query(x)\n",
    "    keys = self.W_key(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    values = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    queries = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "    keys = keys.transpose(1, 2)\n",
    "    values = values.transpose(1, 2)\n",
    "    queries = queries.transpose(1, 2)\n",
    "\n",
    "    attn_score = queries @ keys.transpose(2, 3)\n",
    "\n",
    "    attn_score.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "    attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vectors = (attn_weights @ values).transpose(1, 2)\n",
    "    context_vectors = context_vectors.contiguous().view(\n",
    "                                              b, num_tokens, self.dim_out_emb)\n",
    "    context_vectors = self.out_proj(context_vectors)\n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kxLDpqYm0-EV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1e1d0975-da3e-4c5d-e6a3-af015ba8db46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 2, 1]),)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(132)\n",
    "batch_size, context_length, dim_in_emb = batch.shape #([2, 6, 3])\n",
    "dim_out_emb = 2\n",
    "mha_optimized = MultiHeadAttention_opt(\n",
    "    dim_in_emb, dim_out_emb, context_length, 0.0, num_heads=2)\n",
    "context_vectors = mha_optimized(batch)\n",
    "context_vectors.shape, #context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMuRVwnf-S0V"
   },
   "source": [
    "**Transformer Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxorP3uh1wTQ"
   },
   "outputs": [],
   "source": [
    "''' Transformer architecture\n",
    "1 -Layer normalization to ensure each layer's output have consistent mean/var\n",
    "during training 2. - Shortcut connections to skip one or more layers feeding\n",
    "the output of one layer directly to mitigate vanishing gradients during training\n",
    "3. Transformer blocks combining masked multi-head modules and fully connected\n",
    "FF networks using GELU - 4. Configuration for the GPT model parameters\n",
    "5. - The text generation capability of GPT to decode output tensors into text by\n",
    "sequentially predicting a token one at a time based on given input/context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iUqExKND1oG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LShhqYPofrEg"
   },
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768,\n",
    "    'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "    self.token_embedding_layer = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "    self.pos_embedding_layer = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "    self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "    self.transformer_blocks = nn.Sequential(\n",
    "        *(TransformerBlock(cfg) for _ in range(cfg['n_layers'])))\n",
    "    self.final_normalization = LayerNormalization(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    tokens_embedding = self.token_embedding_layer(in_idx)\n",
    "    positional_embeddings = self.pos_embedding_layer(\n",
    "        torch.arange(seq_len, device=in_idx.device))\n",
    "    x = tokens_embedding + positional_embeddings\n",
    "    x = self.drop_embedding(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "    x = self.final_normalization(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s7NID3E-8zy",
    "outputId": "85edd83a-13d1-453c-ca35-1d810d52f29b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "batch = []\n",
    "text_1 = 'The Project Gutenberg eBook of Anna Karenina by Leo Tolstoy'\n",
    "text_2 = 'License included with this eBook or online www.gutenberg.org'\n",
    "batch.append(torch.tensor(tokenizer.encode(text_1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text_2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "len(batch[0]), len(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLkxETjD-84k",
    "outputId": "6865a9f3-c487-4434-ff00-e72c59d28944"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (token_embedding_layer): Embedding(50257, 768)\n",
       "  (pos_embedding_layer): Embedding(1024, 768)\n",
       "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock()\n",
       "    (1): TransformerBlock()\n",
       "    (2): TransformerBlock()\n",
       "    (3): TransformerBlock()\n",
       "    (4): TransformerBlock()\n",
       "    (5): TransformerBlock()\n",
       "    (6): TransformerBlock()\n",
       "    (7): TransformerBlock()\n",
       "    (8): TransformerBlock()\n",
       "    (9): TransformerBlock()\n",
       "    (10): TransformerBlock()\n",
       "    (11): TransformerBlock()\n",
       "  )\n",
       "  (final_normalization): LayerNormalization()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(Config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob6oiyMBfrKJ",
    "outputId": "30e0a235-a0dc-46da-f441-6962e082b255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 13, 50257]),)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(batch)\n",
    "logits.shape, #logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hN14EBnxLN6m",
    "outputId": "e834fee7-0d2e-4a59-a30e-f98fb5c12ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2436, 0.9998, 0.0000, 1.3965, 0.0000, 0.0000],\n",
       "         [0.5880, 0.0000, 0.0578, 0.0000, 0.0000, 0.0000]],\n",
       "        grad_fn=<ReluBackward0>),\n",
       " tensor([[0.4400],\n",
       "         [0.1076]], grad_fn=<MeanBackward1>),\n",
       " tensor([[0.3695],\n",
       "         [0.0559]], grad_fn=<VarBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' normalizing activations w/ layer normalization'''\n",
    "# generate the data\n",
    "batch_ = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_)\n",
    "mean, var = out.mean(dim=-1, keepdim=True), out.var(dim=-1, keepdim=True)\n",
    "out, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pb7nxYRbLN-M",
    "outputId": "2d0f154f-13e1-4061-c336-b976ec3624b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3230,  0.9209, -0.7238,  1.5735, -0.7238, -0.7238],\n",
       "        [ 2.0315, -0.4552, -0.2108, -0.4552, -0.4552, -0.4552]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "out_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQVfdux8LOBN",
    "outputId": "48e4f1c9-c61c-471d-995e-cb5cd890e6e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1374, -0.4933,  1.8258,  0.1372, -0.3324],\n",
       "         [ 0.6198,  1.0866, -0.3940,  0.4435, -1.7559]], grad_fn=<AddBackward0>),\n",
       " tensor([[0.0000e+00],\n",
       "         [5.9605e-09]], grad_fn=<MeanBackward1>),\n",
       " tensor([[1.0000],\n",
       "         [1.0000]], grad_fn=<VarBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    norm_x = (x -mean) / torch.sqrt(var + self.eps)\n",
    "    return self.scale * norm_x + self.shift\n",
    "\n",
    "layer_norm = LayerNormalization(emb_dim=5)\n",
    "out_layer_norm = layer_norm(batch_)\n",
    "mean = out_layer_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_layer_norm.var(dim=-1, keepdim=True, unbiased=False)\n",
    "out_layer_norm, mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsVzpUSuPKVl"
   },
   "source": [
    "**Implementing FeedForward with GeLU activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "EZkmlAnPNQx9",
    "outputId": "96abf424-6e38-4e40-9200-5c52ba6d8d20"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self,x):\n",
    "    return 0.5* x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.0447716 * torch.pow(x,3))\n",
    "    ))\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3,3,100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (6,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], ['GELU', 'ReLU']), 1):\n",
    "  plt.subplot(1, 2, i)\n",
    "  plt.plot(x,y)\n",
    "  plt.title(f'{label}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOzsfwYlNQ1c",
    "outputId": "19fa5948-3c2d-4ebc-86cb-1962775a1914"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 768]), 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim']),\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.layers(x)\n",
    "\n",
    "ff = FeedForward(Config)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ff(x)\n",
    "out.shape, Config['emb_dim'], #out,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo_f9w1yYqPE"
   },
   "source": [
    "Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8K12YgxY4kI"
   },
   "outputs": [],
   "source": [
    "''' Neural network with shortcut connections '''\n",
    "class DeepNeuralNetowork(nn.Module):\n",
    "  def __init__(self, layer_sizes, use_shortcut):\n",
    "    super().__init__()\n",
    "    self.use_shortcut = use_shortcut\n",
    "    self.layers = nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "    ])\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      layer_out = layer(x)\n",
    "      if self.use_shortcut and x.shape == layer_out.shape:\n",
    "        x = x + layer_out\n",
    "      else:\n",
    "        x = layer_out\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HPFOjUcbBJN",
    "outputId": "1bc1b28a-f244-49da-9baf-6adce4846da5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 4.9807589675765485e-05\n",
      "layers.0.0.bias has gradient mean of 7.471138815162703e-05\n",
      "layers.1.0.weight has gradient mean of 0.00011093103239545599\n",
      "layers.1.0.bias has gradient mean of 0.0008753305301070213\n",
      "layers.2.0.weight has gradient mean of 0.0006009326316416264\n",
      "layers.2.0.bias has gradient mean of 0.004141479264944792\n",
      "layers.3.0.weight has gradient mean of 0.001894055982120335\n",
      "layers.3.0.bias has gradient mean of 0.013888943009078503\n",
      "layers.4.0.weight has gradient mean of 0.008871190249919891\n",
      "layers.4.0.bias has gradient mean of 0.051304250955581665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "input = torch.tensor([1., 0., -1.])\n",
    "torch.manual_seed(132)\n",
    "mwsh = DeepNeuralNetowork(layer_sizes, use_shortcut=False)\n",
    "def gradients(model, x):\n",
    "  output = model(x)\n",
    "  target = torch.tensor([[0.]])\n",
    "  loss = nn.MSELoss()\n",
    "  loss = loss(output,target)\n",
    "  loss.backward()\n",
    "  for name, param in model.named_parameters():\n",
    "    if 'weight':\n",
    "      print(f'{name} has gradient mean of {param.grad.abs().mean().item()}')\n",
    "gradients(mwsh, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5kfvOnFbDbS",
    "outputId": "fd4982ac-38a8-4de7-a979-bb2e57d5b9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.003889854997396469\n",
      "layers.0.0.bias has gradient mean of 0.005834782496094704\n",
      "layers.1.0.weight has gradient mean of 0.003715590573847294\n",
      "layers.1.0.bias has gradient mean of 0.005611700937151909\n",
      "layers.2.0.weight has gradient mean of 0.009199644438922405\n",
      "layers.2.0.bias has gradient mean of 0.013666793704032898\n",
      "layers.3.0.weight has gradient mean of 0.005073778331279755\n",
      "layers.3.0.bias has gradient mean of 0.009616758674383163\n",
      "layers.4.0.weight has gradient mean of 0.038982097059488297\n",
      "layers.4.0.bias has gradient mean of 0.052308082580566406\n"
     ]
    }
   ],
   "source": [
    "mwsh = DeepNeuralNetowork(layer_sizes, use_shortcut=True)\n",
    "gradients(mwsh, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8zUgchzkD5C"
   },
   "source": [
    "Attention and linear layers in  transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yqmcKV0bDfl"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.attention = MultiHeadAttention_opt(\n",
    "        dim_in_emb = cfg['emb_dim'],\n",
    "        dim_out_emb = cfg['emb_dim'],\n",
    "        context_length = cfg['context_length'],\n",
    "        num_heads = cfg['n_heads'],\n",
    "        dropout = cfg['drop_rate'],\n",
    "        qkv_bias = cfg['qkv_bias'])\n",
    "    self.ff = FeedForward(cfg)\n",
    "    self.norm1 = LayerNormalization(cfg['emb_dim'])\n",
    "    self.norm2 = LayerNormalization(cfg['emb_dim'])\n",
    "    self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "  def forward(self, x):\n",
    "    shortcut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.attention(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    return x\n",
    "\n",
    "x = torch.rand(2, 4, 768)\n",
    "trnf_block = TransformerBlock(Config)\n",
    "out = trnf_block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1n1opPqjNRBm"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "    self.positional_embedding = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "    self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "    self.transformer_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "    self.final_normalization = LayerNormalization(cfg['emb_dim'])\n",
    "    self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False )\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    token_embedding = self.token_embedding(in_idx)\n",
    "    positional_embedding = self.positional_embedding(\n",
    "        torch.arange(seq_len, device=in_idx.device) )\n",
    "    x = token_embedding + positional_embedding\n",
    "    x = self.drop_embedding(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "    x = self.final_normalization(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lcBf_T_7nFX"
   },
   "outputs": [],
   "source": [
    "model_gpt = GPT(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPEpNrkmsXjN",
    "outputId": "82801b0a-91d1-4e00-890f-6cf07a4651d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 13, 50257]),\n",
       " tensor([[[ 0.9998, -0.6334, -0.2243,  ..., -0.7347, -0.0905, -0.2656],\n",
       "          [-0.4434,  0.1494, -0.9669,  ..., -0.6166, -0.6777,  1.6482],\n",
       "          [-0.0440,  0.6219,  0.7871,  ..., -0.0170, -0.3031,  0.4117],\n",
       "          ...,\n",
       "          [ 0.7793,  0.0108,  0.9134,  ...,  1.5485, -0.0767, -0.1357],\n",
       "          [ 0.5521,  2.1198, -0.0658,  ..., -1.6713, -0.0582,  0.6999],\n",
       "          [-1.0729,  0.3633, -1.2405,  ...,  0.8049,  0.0288,  0.4847]],\n",
       " \n",
       "         [[ 1.1473, -0.9356,  0.2056,  ..., -0.8087, -0.6197,  0.2240],\n",
       "          [ 0.4987,  0.4062, -0.2237,  ...,  0.2348,  1.5309,  0.6760],\n",
       "          [ 0.2612,  0.0294,  1.6646,  ..., -1.1115, -1.2451, -0.5173],\n",
       "          ...,\n",
       "          [ 0.6197, -0.0715,  1.1852,  ...,  0.4778, -0.4336,  0.1047],\n",
       "          [ 0.1189, -0.4237,  0.4810,  ..., -0.1871,  1.0509,  0.7817],\n",
       "          [-0.3464,  0.4167, -0.9144,  ...,  1.0888, -0.0550,  0.7064]]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(batch)\n",
    "out.shape, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XY_s_14s0SOO",
    "outputId": "48048e19-dbe4-4456-a868-03a64db2574f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163009536, 621.83203125)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt.parameters()\n",
    "total_params = sum([param.numel() for param in model_gpt.parameters()])\n",
    "total_size = total_params*4/(1024*1024)\n",
    "total_params, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ak0SoSTQ0SR_",
    "outputId": "52bb56d4-edd7-470b-8efe-1a055d5abeb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 768]), torch.Size([50257, 768]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt.token_embedding.weight.shape, model_gpt.out_head.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX9OfFN_BPPn"
   },
   "source": [
    "Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkoxI4M60SWL"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, idx, max_new_tokens, context_size):\n",
    "  for _ in range(max_new_tokens):\n",
    "    idx_c = idx[:, -context_size:]\n",
    "    with torch.no_grad():\n",
    "      logits = model(idx_c)\n",
    "    logits = logits[:, -1, :]\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "  return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OLNoXfkGzGx",
    "outputId": "b90596ed-22af-4f60-9fbd-3a6da876f111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5]), [5661, 318, 655, 617, 2420])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode('this is just some text')\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "encoded_tensor.shape, encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWofUo2vMoHY",
    "outputId": "1748ed11-7d07-43df-92f3-07eed4709565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5661,   318,   655,   617,  2420, 18144,  8080, 17516, 39620, 33128,\n",
       "          3731, 45370, 29908, 37743, 41087]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "text_out = generate_text(\n",
    "    model=model_gpt,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = Config['context_length'])\n",
    "text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tihh4ytUMoLq",
    "outputId": "75957c5b-1307-412d-aa8d-74dba40d0c3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'this is just some textamera sustain volcanographed Abbey slight embodies wrapper humiliating Cheap'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' without training the model generates incoherent text  '''\n",
    "decoded_text_out = tokenizer.decode(text_out.squeeze(0).tolist())\n",
    "decoded_text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EwXuQxx0SZj"
   },
   "outputs": [],
   "source": [
    "def text_to_tokenids(text, tokenizer):\n",
    "  encoded = tokenizer.encode(text) #, allowed_special='<|endoftext|>')\n",
    "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "  return encoded_tensor\n",
    "\n",
    "def tokenids_to_text(tokenids, tokenizer):\n",
    "  return tokenizer.decode(tokenids.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJlY1t4X9kub"
   },
   "outputs": [],
   "source": [
    "file_path = 'ebook.txt'\n",
    "#url = 'https://www.gutenberg.org/cache/epub/514/pg514.txt'\n",
    "url = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "import os\n",
    "import urllib.request\n",
    "with urllib.request.urlopen(url) as response:\n",
    "  text_ebook = response.read().decode('utf-8')\n",
    "with open(file_path, 'w', encoding = 'utf-8') as fd:\n",
    "  fd.write(text_ebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsNHM7Cg9kya"
   },
   "outputs": [],
   "source": [
    "with open('ebook.txt', 'r', encoding = 'utf-8') as fd:\n",
    "  text_ebook = fd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uqr03uSh9k31"
   },
   "outputs": [],
   "source": [
    "with open('ebook2.txt', 'r', encoding = 'utf-8') as fd:\n",
    "  text_ebook = fd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mOIb6bB9k_f"
   },
   "outputs": [],
   "source": [
    "tokenized_ebooks = tokenizer.encode(text_ebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhoskR0B9lDg",
    "outputId": "976cfe86-f31d-4dc0-e457-d0e4e20d2067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286183, 1026358)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_ebooks), len(text_ebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIk-E69JM9St"
   },
   "source": [
    "Training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhIdrFm1M8bZ"
   },
   "outputs": [],
   "source": [
    "train_split = 0.90\n",
    "split_idx = int(train_split * len(text_ebook))\n",
    "train_data = text_ebook[:split_idx]\n",
    "val_data = text_ebook[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7Bl_Hmg9lJL"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset_v1(\n",
    "  train_data, tokenizer,\n",
    "  max_length = Config['context_length'], stride= Config['context_length'])\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, batch_size = batch_size, drop_last = True, shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC2Jnn1C9lNK"
   },
   "outputs": [],
   "source": [
    "val_dataset = Dataset_v1(\n",
    "  val_data, tokenizer,\n",
    "  max_length = Config['context_length'], stride= Config['context_length'])\n",
    "\n",
    "batch_size = 4\n",
    "val_loader = DataLoader(\n",
    "  val_dataset, batch_size = batch_size, drop_last = True, shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-8Xsjuz0SxQ",
    "outputId": "b98f9867-2434-44f3-9bcf-1f8fdfe09fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7dee5bf452d0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7dee5a2e04d0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9l8nIkvnQYZF",
    "outputId": "d73645ca-b2c6-4247-a9c7-31d92a2b77b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6k3D-SPQYe8",
    "outputId": "c41341f5-f329-4032-b93f-142e44b96e72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258048, 24576, 282624)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "  train_tokens += input_batch.numel()\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "  val_tokens += input_batch.numel()\n",
    "train_tokens, val_tokens, train_tokens + val_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVQL_zz0QYiE"
   },
   "outputs": [],
   "source": [
    "def loss_batch(input_batch, target_batch, model, device):\n",
    "  input_batch = input_batch.to(device)\n",
    "  target_batch = target_batch.to(device)\n",
    "  logits = model(input_batch)\n",
    "  loss = torch.nn.functional.cross_entropy(\n",
    "      logits.flatten(0,1), target_batch.flatten() )\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SW-YsUQfTk32"
   },
   "outputs": [],
   "source": [
    "def loss_loader(data_loader, model, device, num_batches=None):\n",
    "  tloss = 0\n",
    "  if len(data_loader) == 0:\n",
    "    return float('nan')\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    if i < num_batches:\n",
    "      loss = loss_batch(input_batch, target_batch, model, device)\n",
    "      tloss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "  # sums the loss for each batch, and averages the loss over allbatches\n",
    "  return tloss/num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m83b1KdOkevZ"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcLeGnrBlRjq",
    "outputId": "89c274ff-9943-45f6-a718-ced52a1b61ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (token_embedding_layer): Embedding(50257, 768)\n",
       "  (pos_embedding_layer): Embedding(1024, 768)\n",
       "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock()\n",
       "    (1): TransformerBlock()\n",
       "    (2): TransformerBlock()\n",
       "    (3): TransformerBlock()\n",
       "    (4): TransformerBlock()\n",
       "    (5): TransformerBlock()\n",
       "    (6): TransformerBlock()\n",
       "    (7): TransformerBlock()\n",
       "    (8): TransformerBlock()\n",
       "    (9): TransformerBlock()\n",
       "    (10): TransformerBlock()\n",
       "    (11): TransformerBlock()\n",
       "  )\n",
       "  (final_normalization): LayerNormalization()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCCTO2v2Tk70",
    "outputId": "ba765b20-08c6-4e9d-94d7-e391006366ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.001023898049007, 5.628655751546224)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "  train_loss = loss_loader(train_loader, model, device)\n",
    "  val_loss = loss_loader(val_loader, model, device)\n",
    "\n",
    "train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqXk7ULoqDCq"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftiu5nFmu-MA"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "  with torch.no_grad():\n",
    "    train_loss = loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss = loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "  model.train()\n",
    "  return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3zc6r6vr5Un"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer,device, num_epochs,\n",
    "      eval_freq, eval_iter, tokenizer): #start_context\n",
    "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "  tokens_seen, global_step = 0, -1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_batch(input_batch, target_batch, model, device)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      tokens_seen += input_batch.numel()\n",
    "      global_step +=1\n",
    "\n",
    "      if global_step % eval_freq ==0:\n",
    "        train_loss, val_loss = evaluate_model(\n",
    "            model, train_loader, val_loader, device, eval_iter\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        track_tokens_seen.append(tokens_seen)\n",
    "        print(f'epoch {epoch +1}, step {global_step}, train loss {train_loss:.3f}, val loss {val_loss:.3f}')\n",
    "\n",
    "    #generate_and_print_sample(model, tokenizer, device,start_context)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToCa6E1VhV7A"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "torch.manual_seed(132)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr = 0.0004, weight_decay = 0.1 )\n",
    "num_epochs = 10\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "  eval_freq=15, eval_iter=1, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX0uG4NjhVps"
   },
   "outputs": [],
   "source": [
    "def generate_content(model, tokenizer, device, start_context):\n",
    "  model.eval()\n",
    "  context_size = model.pos_embedding_layer.weight.shape[0]\n",
    "  encoded = text_to_tokenids(start_context, tokenizer).to(device)\n",
    "  with torch.no_grad():\n",
    "    token_ids = generate_text(model = model,\n",
    "            idx=encoded, max_new_tokens = 50, context_size = context_size)\n",
    "    decoded_text = tokenids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace('\\n', ''))\n",
    "  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kN-88HCw01r"
   },
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "start_context = \"journey\"\n",
    "generate_content(model, tokenizer, device, start_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECvaH-EXhWI7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRHXtusCz2Zp"
   },
   "source": [
    "Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbwHz2zYptsR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWSZXSs4zRbT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULv6774fptw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3diqv33z9-k"
   },
   "source": [
    "Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXWw58FLpt1y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1I7nB-FNRFq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
